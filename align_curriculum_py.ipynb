{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNxDT9j5T5EAus59YuC+mxY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Pinn/blob/main/align_curriculum_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9BM4qxUJQl0"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Title: Multimodal AGI Alignment Curriculum (InfoNCE + Prototypes)\n",
        "Author: Copilot\n",
        "Provenance: Self-contained demo for multimodal contrastive alignment with a simple curriculum.\n",
        "License: MIT\n",
        "\n",
        "Overview:\n",
        "- Synthetic multimodal \"class\" alignment: text tokens <-> generated image sprites\n",
        "- Curriculum stages increase class count, image noise, and distractors\n",
        "- InfoNCE (NT-Xent) symmetric loss with learnable temperature\n",
        "- Optional prototype memory (simple world-model-ish prior) for class centroids\n",
        "- CSV logging and checkpointing\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities and configuration\n",
        "# ---------------------------\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AgentConfig:\n",
        "    name: str = \"MyAGIAgent\"\n",
        "    notes: str = \"Multimodal InfoNCE with curriculum\"\n",
        "    device: str = \"cpu\"\n",
        "    seed: int = 1337\n",
        "\n",
        "    # Model dims\n",
        "    text_vocab_size: int = 1024  # upper bound, curriculum stages pick <= this\n",
        "    text_embed_dim: int = 128\n",
        "    text_hidden_dim: int = 256\n",
        "    img_channels: int = 1\n",
        "    img_size: int = 28\n",
        "    proj_dim: int = 128\n",
        "\n",
        "    # Training\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    max_grad_norm: float = 1.0\n",
        "    temperature_init: float = 0.07  # typical starting temp for contrastive\n",
        "    use_world_model: bool = True    # enable prototype memory term\n",
        "    proto_momentum: float = 0.9     # EMA update for prototypes\n",
        "    proto_weight: float = 0.1       # weight for prototype consistency loss\n",
        "\n",
        "    # Curriculum\n",
        "    stages: int = 3\n",
        "    steps_per_stage: int = 300\n",
        "    eval_interval: int = 50\n",
        "    batch_size: int = 64\n",
        "\n",
        "    # Logging/checkpoints\n",
        "    log_dir: str = \"./runs\"\n",
        "    save_every_stage: bool = True\n",
        "\n",
        "\n",
        "class AlignmentLogger:\n",
        "    def __init__(self, log_dir: str, run_name: str):\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        self.dir = os.path.join(log_dir, f\"{run_name}_{ts}\")\n",
        "        os.makedirs(self.dir, exist_ok=True)\n",
        "        self.csv_path = os.path.join(self.dir, \"metrics.csv\")\n",
        "        self._init_csv()\n",
        "        self.last_print = time.time()\n",
        "\n",
        "    def _init_csv(self):\n",
        "        with open(self.csv_path, \"w\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow([\n",
        "                \"step\", \"stage\", \"loss\", \"loss_infonce\", \"loss_proto\",\n",
        "                \"acc@1_text2img\", \"acc@1_img2text\", \"temperature\", \"lr\"\n",
        "            ])\n",
        "\n",
        "    def log(self, step: int, stage: int, metrics: Dict[str, Any]):\n",
        "        with open(self.csv_path, \"a\", newline=\"\") as f:\n",
        "            w = csv.writer(f)\n",
        "            w.writerow([\n",
        "                step, stage,\n",
        "                float(metrics.get(\"loss\", float(\"nan\"))),\n",
        "                float(metrics.get(\"loss_infonce\", float(\"nan\"))),\n",
        "                float(metrics.get(\"loss_proto\", 0.0)),\n",
        "                float(metrics.get(\"acc_t2i\", float(\"nan\"))),\n",
        "                float(metrics.get(\"acc_i2t\", float(\"nan\"))),\n",
        "                float(metrics.get(\"temperature\", float(\"nan\"))),\n",
        "                float(metrics.get(\"lr\", float(\"nan\"))),\n",
        "            ])\n",
        "\n",
        "    def console(self, step: int, stage: int, metrics: Dict[str, Any], throttle_sec: float = 1.0):\n",
        "        now = time.time()\n",
        "        if now - self.last_print >= throttle_sec:\n",
        "            msg = (\n",
        "                f\"[stage {stage} | step {step}] \"\n",
        "                f\"loss={metrics.get('loss', 'NA'):.4f} \"\n",
        "                f\"(infonce={metrics.get('loss_infonce','NA'):.4f}, proto={metrics.get('loss_proto',0.0):.4f}) \"\n",
        "                f\"acc@1 t2i={metrics.get('acc_t2i','NA'):.3f} i2t={metrics.get('acc_i2t','NA'):.3f} \"\n",
        "                f\"T={metrics.get('temperature','NA'):.4f} lr={metrics.get('lr','NA'):.2e}\"\n",
        "            )\n",
        "            print(msg, flush=True)\n",
        "            self.last_print = now\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Synthetic multimodal dataset\n",
        "# ---------------------------\n",
        "\n",
        "def generate_sprite(class_id: int, size: int, variant: int, noise: float, device: str) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Creates a simple 2D sprite pattern deterministically from class_id and variant.\n",
        "    Patterns: vertical/horizontal bars, diagonals, blocks; class_id controls which and where.\n",
        "    \"\"\"\n",
        "    img = torch.zeros((size, size), dtype=torch.float32)\n",
        "    cid = class_id % 8\n",
        "    rng = (class_id * 131 + variant * 17) % (size - 2) + 1\n",
        "\n",
        "    if cid == 0:\n",
        "        img[:, rng] = 1.0\n",
        "    elif cid == 1:\n",
        "        img[rng, :] = 1.0\n",
        "    elif cid == 2:\n",
        "        for i in range(size):\n",
        "            j = (i + rng) % size\n",
        "            img[i, j] = 1.0\n",
        "    elif cid == 3:\n",
        "        for i in range(size):\n",
        "            j = (rng - i) % size\n",
        "            img[i, j] = 1.0\n",
        "    elif cid == 4:\n",
        "        img[rng-1:rng+1, rng-1:rng+1] = 1.0\n",
        "    elif cid == 5:\n",
        "        img[rng-1:rng+1, :] = 1.0\n",
        "    elif cid == 6:\n",
        "        img[:, rng-1:rng+1] = 1.0\n",
        "    else:\n",
        "        # checkerboard-ish\n",
        "        for i in range(0, size, 2):\n",
        "            for j in range((i + rng) % 2, size, 2):\n",
        "                img[i, j] = 1.0\n",
        "\n",
        "    # normalize pattern and add noise\n",
        "    img = img + noise * torch.randn_like(img)\n",
        "    img = (img - img.mean()) / (img.std() + 1e-6)\n",
        "    return img.to(device)\n",
        "\n",
        "\n",
        "def make_text_sequence(class_id: int, vocab_offset: int = 2) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Creates a tiny sequence: [BOS(0), class_token(vocab_offset + class_id), EOS(1)].\n",
        "    \"\"\"\n",
        "    return torch.tensor([0, vocab_offset + class_id, 1], dtype=torch.long)\n",
        "\n",
        "\n",
        "def sample_batch(\n",
        "    batch_size: int,\n",
        "    num_classes: int,\n",
        "    img_size: int,\n",
        "    noise: float,\n",
        "    device: str,\n",
        "    vocab_offset: int = 2\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      text_tokens: (B, L) with token ids\n",
        "      images: (B, 1, H, W)\n",
        "      labels: (B,) class ids [0..num_classes-1]\n",
        "    \"\"\"\n",
        "    labels = torch.randint(low=0, high=num_classes, size=(batch_size,), device=device)\n",
        "    # build sequences\n",
        "    seqs = [make_text_sequence(int(c), vocab_offset) for c in labels.cpu().tolist()]\n",
        "    L = len(seqs[0])\n",
        "    text_tokens = torch.stack(seqs, dim=0).to(device)  # (B, L)\n",
        "\n",
        "    # images\n",
        "    imgs = []\n",
        "    for i, c in enumerate(labels.cpu().tolist()):\n",
        "        variant = i  # distinct within batch\n",
        "        sprite = generate_sprite(c, size=img_size, variant=variant, noise=noise, device=device)\n",
        "        imgs.append(sprite.unsqueeze(0))  # (1, H, W)\n",
        "    images = torch.stack(imgs, dim=0).to(device)  # (B, 1, H, W)\n",
        "    return text_tokens, images, labels\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Encoders and agent\n",
        "# ---------------------------\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, proj_dim: int):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.proj = nn.Linear(hidden_dim, proj_dim)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.embed(tokens)               # (B, L, E)\n",
        "        _, h = self.gru(x)                   # (1, B, H)\n",
        "        h = h.squeeze(0)                     # (B, H)\n",
        "        z = self.proj(h)                     # (B, D)\n",
        "        z = F.normalize(z, dim=-1)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, in_ch: int, proj_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, 32, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),                 # 14x14 for 28x28\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),                 # 7x7\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),    # 128x1x1\n",
        "        )\n",
        "        self.proj = nn.Linear(128, proj_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.net(x).flatten(1)           # (B, 128)\n",
        "        z = self.proj(h)                     # (B, D)\n",
        "        z = F.normalize(z, dim=-1)\n",
        "        return z\n",
        "\n",
        "\n",
        "class MyAGIAgent(nn.Module):\n",
        "    def __init__(self, cfg: AgentConfig, vocab_size_cap: int):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.text_enc = TextEncoder(\n",
        "            vocab_size=vocab_size_cap,\n",
        "            embed_dim=cfg.text_embed_dim,\n",
        "            hidden_dim=cfg.text_hidden_dim,\n",
        "            proj_dim=cfg.proj_dim\n",
        "        )\n",
        "        self.img_enc = ImageEncoder(\n",
        "            in_ch=cfg.img_channels,\n",
        "            proj_dim=cfg.proj_dim\n",
        "        )\n",
        "        # learnable temperature (clamped)\n",
        "        self.logit_scale = nn.Parameter(torch.tensor(math.log(1.0 / cfg.temperature_init)))\n",
        "        # prototype memory: class_id -> vector\n",
        "        self.prototypes: Optional[torch.Tensor] = None  # (num_classes, D)\n",
        "\n",
        "    def reset_prototypes(self, num_classes: int, device: str):\n",
        "        self.prototypes = F.normalize(torch.randn(num_classes, self.cfg.proj_dim, device=device), dim=-1)\n",
        "\n",
        "    def temperature(self) -> torch.Tensor:\n",
        "        # temperature = exp(-logit_scale)\n",
        "        return torch.exp(-self.logit_scale).clamp(1e-3, 1.0)\n",
        "\n",
        "    def infonce_loss(self, z_t: torch.Tensor, z_i: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "        # cosine similarities scaled by 1/T\n",
        "        T = self.temperature()\n",
        "        logits = (z_t @ z_i.t()) / T\n",
        "        labels = torch.arange(z_t.size(0), device=z_t.device)\n",
        "        loss_t2i = F.cross_entropy(logits, labels)\n",
        "        loss_i2t = F.cross_entropy(logits.t(), labels)\n",
        "        loss = 0.5 * (loss_t2i + loss_i2t)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            acc_t2i = (logits.argmax(dim=1) == labels).float().mean().item()\n",
        "            acc_i2t = (logits.t().argmax(dim=1) == labels).float().mean().item()\n",
        "\n",
        "        return loss, {\n",
        "            \"loss_infonce\": float(loss.item()),\n",
        "            \"acc_t2i\": float(acc_t2i),\n",
        "            \"acc_i2t\": float(acc_i2t),\n",
        "            \"temperature\": float(T.item()),\n",
        "        }\n",
        "\n",
        "    def prototype_loss(self, z_t: torch.Tensor, z_i: torch.Tensor, class_ids: torch.Tensor) -> torch.Tensor:\n",
        "        # encourage both modalities to be close to class prototype\n",
        "        if self.prototypes is None:\n",
        "            return torch.tensor(0.0, device=z_t.device)\n",
        "\n",
        "        p = self.prototypes[class_ids]  # (B, D)\n",
        "        # cosine distance -> 1 - cosine similarity\n",
        "        d_t = 1.0 - (z_t * p).sum(dim=-1)\n",
        "        d_i = 1.0 - (z_i * p).sum(dim=-1)\n",
        "        return (d_t.mean() + d_i.mean()) * 0.5\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_prototypes(self, class_ids: torch.Tensor, z_t: torch.Tensor, z_i: torch.Tensor):\n",
        "        if self.prototypes is None:\n",
        "            return\n",
        "        momentum = self.cfg.proto_momentum\n",
        "        z = F.normalize((z_t + z_i) * 0.5, dim=-1)  # fused representation\n",
        "        # EMA update per class present in batch\n",
        "        for cls in class_ids.unique():\n",
        "            mask = (class_ids == cls)\n",
        "            if mask.any():\n",
        "                mean_z = F.normalize(z[mask].mean(dim=0), dim=-1)\n",
        "                self.prototypes[cls] = F.normalize(momentum * self.prototypes[cls] + (1 - momentum) * mean_z, dim=-1)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, images: torch.Tensor, class_ids: torch.Tensor) -> Dict[str, Any]:\n",
        "        z_t = self.text_enc(tokens)     # (B, D)\n",
        "        z_i = self.img_enc(images)      # (B, D)\n",
        "\n",
        "        loss_infonce, stats = self.infonce_loss(z_t, z_i)\n",
        "        loss_proto = torch.tensor(0.0, device=images.device)\n",
        "        if self.cfg.use_world_model:\n",
        "            loss_proto = self.prototype_loss(z_t, z_i, class_ids) * self.cfg.proto_weight\n",
        "\n",
        "        loss = loss_infonce + loss_proto\n",
        "        out = {\"loss\": loss, \"loss_infonce\": loss_infonce, \"loss_proto\": loss_proto}\n",
        "        out.update(stats)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Adapter\n",
        "# ---------------------------\n",
        "\n",
        "class AgentAdapter:\n",
        "    def __init__(self, agent: MyAGIAgent, cfg: AgentConfig, optimizer: torch.optim.Optimizer, scheduler=None):\n",
        "        self.agent = agent\n",
        "        self.cfg = cfg\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train_step(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Dict[str, Any]:\n",
        "        self.agent.train()\n",
        "        tokens, images, labels = batch\n",
        "        out = self.agent(tokens, images, labels)\n",
        "        loss: torch.Tensor = out[\"loss\"]\n",
        "        self.optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.agent.parameters(), self.cfg.max_grad_norm)\n",
        "        self.optimizer.step()\n",
        "        if self.scheduler is not None:\n",
        "            self.scheduler.step()\n",
        "        # prototype update after step (target moves with model)\n",
        "        if self.agent.cfg.use_world_model:\n",
        "            with torch.no_grad():\n",
        "                z_t = self.agent.text_enc(tokens)\n",
        "                z_i = self.agent.img_enc(images)\n",
        "                self.agent.update_prototypes(labels, z_t, z_i)\n",
        "        # collect scalars\n",
        "        metrics = {\n",
        "            \"loss\": float(out[\"loss\"].item()),\n",
        "            \"loss_infonce\": float(out[\"loss_infonce\"].item()),\n",
        "            \"loss_proto\": float(out[\"loss_proto\"].item()),\n",
        "            \"acc_t2i\": float(out[\"acc_t2i\"]),\n",
        "            \"acc_i2t\": float(out[\"acc_i2t\"]),\n",
        "            \"temperature\": float(out[\"temperature\"]),\n",
        "            \"lr\": float(self.optimizer.param_groups[0][\"lr\"])\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Dict[str, Any]:\n",
        "        self.agent.eval()\n",
        "        tokens, images, labels = batch\n",
        "        out = self.agent(tokens, images, labels)\n",
        "        metrics = {\n",
        "            \"loss\": float(out[\"loss\"].item()),\n",
        "            \"loss_infonce\": float(out[\"loss_infonce\"].item()),\n",
        "            \"loss_proto\": float(out[\"loss_proto\"].item()),\n",
        "            \"acc_t2i\": float(out[\"acc_t2i\"]),\n",
        "            \"acc_i2t\": float(out[\"acc_i2t\"]),\n",
        "            \"temperature\": float(out[\"temperature\"]),\n",
        "            \"lr\": float(self.optimizer.param_groups[0][\"lr\"])\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Curriculum trainer\n",
        "# ---------------------------\n",
        "\n",
        "def get_stage_params(stage_idx: int) -> Dict[str, Any]:\n",
        "    # progressively harder: more classes, more noise\n",
        "    stage_specs = [\n",
        "        {\"num_classes\": 4,  \"noise\": 0.02},\n",
        "        {\"num_classes\": 8,  \"noise\": 0.06},\n",
        "        {\"num_classes\": 16, \"noise\": 0.10},\n",
        "    ]\n",
        "    if stage_idx < len(stage_specs):\n",
        "        return stage_specs[stage_idx]\n",
        "    # fallback if more stages requested\n",
        "    n = 16 * (2 ** (stage_idx - 2))\n",
        "    return {\"num_classes\": min(128, n), \"noise\": min(0.20, 0.10 + 0.02 * (stage_idx - 2))}\n",
        "\n",
        "\n",
        "def create_optimizer(agent: MyAGIAgent, cfg: AgentConfig):\n",
        "    params = [\n",
        "        {\"params\": [p for n, p in agent.named_parameters() if \"logit_scale\" not in n]},\n",
        "        {\"params\": [agent.logit_scale], \"lr\": cfg.lr * 0.1},  # smaller lr for temperature\n",
        "    ]\n",
        "    opt = torch.optim.AdamW(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    return opt\n",
        "\n",
        "\n",
        "def run_curriculum(cfg: AgentConfig):\n",
        "    set_seed(cfg.seed)\n",
        "    device = cfg.device if torch.cuda.is_available() and cfg.device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "    # logging setup\n",
        "    logger = AlignmentLogger(cfg.log_dir, cfg.name)\n",
        "\n",
        "    global_step = 0\n",
        "    vocab_cap = cfg.text_vocab_size\n",
        "    agent = None\n",
        "    adapter = None\n",
        "\n",
        "    for stage in range(cfg.stages):\n",
        "        sp = get_stage_params(stage)\n",
        "        num_classes = sp[\"num_classes\"]\n",
        "        noise = sp[\"noise\"]\n",
        "\n",
        "        # initialize agent lazily with vocab big enough (BOS=0, EOS=1, class tokens up to offset+num_classes-1)\n",
        "        vocab_size = max(2 + num_classes, 4)\n",
        "        vocab_size = min(vocab_size, vocab_cap)\n",
        "        if agent is None:\n",
        "            agent = MyAGIAgent(cfg, vocab_size_cap=vocab_size).to(device)\n",
        "            agent.reset_prototypes(num_classes=num_classes, device=device)\n",
        "            optimizer = create_optimizer(agent, cfg)\n",
        "            adapter = AgentAdapter(agent, cfg, optimizer)\n",
        "        else:\n",
        "            # if number of classes grows, resize prototypes\n",
        "            if agent.prototypes is None or agent.prototypes.size(0) != num_classes:\n",
        "                agent.reset_prototypes(num_classes=num_classes, device=device)\n",
        "\n",
        "        # train loop for this stage\n",
        "        for step in range(cfg.steps_per_stage):\n",
        "            tokens, images, labels = sample_batch(\n",
        "                batch_size=cfg.batch_size,\n",
        "                num_classes=num_classes,\n",
        "                img_size=cfg.img_size,\n",
        "                noise=noise,\n",
        "                device=device\n",
        "            )\n",
        "            metrics = adapter.train_step((tokens, images, labels))\n",
        "            logger.log(global_step, stage, metrics)\n",
        "            logger.console(global_step, stage, metrics, throttle_sec=0.5)\n",
        "            global_step += 1\n",
        "\n",
        "            # periodic eval\n",
        "            if (step + 1) % cfg.eval_interval == 0:\n",
        "                with torch.no_grad():\n",
        "                    etok, eimg, elab = sample_batch(\n",
        "                        batch_size=cfg.batch_size,\n",
        "                        num_classes=num_classes,\n",
        "                        img_size=cfg.img_size,\n",
        "                        noise=noise,\n",
        "                        device=device\n",
        "                    )\n",
        "                    eval_metrics = adapter.eval_batch((etok, eimg, elab))\n",
        "                    logger.log(global_step, stage, {**eval_metrics, \"lr\": metrics[\"lr\"]})\n",
        "                    logger.console(global_step, stage, eval_metrics, throttle_sec=0.0)\n",
        "\n",
        "        # checkpoint end of stage\n",
        "        if cfg.save_every_stage:\n",
        "            ckpt = {\n",
        "                \"config\": asdict(cfg),\n",
        "                \"stage\": stage,\n",
        "                \"model_state\": agent.state_dict(),\n",
        "                \"optimizer_state\": adapter.optimizer.state_dict(),\n",
        "                \"vocab_size_cap\": vocab_size,\n",
        "            }\n",
        "            path = os.path.join(logger.dir, f\"checkpoint_stage{stage}.pt\")\n",
        "            torch.save(ckpt, path)\n",
        "            print(f\"Saved checkpoint: {path}\")\n",
        "\n",
        "    # final save\n",
        "    final_path = os.path.join(logger.dir, \"final_model.pt\")\n",
        "    torch.save({\n",
        "        \"config\": asdict(cfg),\n",
        "        \"model_state\": agent.state_dict(),\n",
        "    }, final_path)\n",
        "    print(f\"Saved final model: {final_path}\")\n",
        "    print(f\"CSV metrics: {logger.csv_path}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# CLI\n",
        "# ---------------------------\n",
        "\n",
        "def parse_args() -> AgentConfig:\n",
        "    p = argparse.ArgumentParser(description=\"Multimodal AGI Alignment Curriculum (InfoNCE + Prototypes)\")\n",
        "    p.add_argument(\"--name\", type=str, default=\"MyAGIAgent\", help=\"Run name for logging\")\n",
        "    p.add_argument(\"--device\", type=str, default=\"cpu\", help=\"cpu or cuda:0\")\n",
        "    p.add_argument(\"--seed\", type=int, default=1337)\n",
        "    p.add_argument(\"--lr\", type=float, default=3e-4)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--stages\", type=int, default=3)\n",
        "    p.add_argument(\"--steps_per_stage\", type=int, default=300)\n",
        "    p.add_argument(\"--eval_interval\", type=int, default=50)\n",
        "    p.add_argument(\"--use_world_model\", action=\"store_true\")\n",
        "    p.add_argument(\"--no_world_model\", dest=\"use_world_model\", action=\"store_false\")\n",
        "    p.set_defaults(use_world_model=True)\n",
        "    p.add_argument(\"--proto_weight\", type=float, default=0.1)\n",
        "    p.add_argument(\"--proto_momentum\", type=float, default=0.9)\n",
        "    p.add_argument(\"--log_dir\", type=str, default=\"./runs\")\n",
        "    p.add_argument(\"--img_size\", type=int, default=28)\n",
        "    p.add_argument(\"--text_vocab_size\", type=int, default=1024)\n",
        "    p.add_argument(\"--temperature_init\", type=float, default=0.07)\n",
        "    args = p.parse_args()\n",
        "\n",
        "    cfg = AgentConfig(\n",
        "        name=args.name,\n",
        "        device=args.device,\n",
        "        seed=args.seed,\n",
        "        lr=args.lr,\n",
        "        batch_size=args.batch_size,\n",
        "        stages=args.stages,\n",
        "        steps_per_stage=args.steps_per_stage,\n",
        "        eval_interval=args.eval_interval,\n",
        "        use_world_model=args.use_world_model,\n",
        "        proto_weight=args.proto_weight,\n",
        "        proto_momentum=args.proto_momentum,\n",
        "        log_dir=args.log_dir,\n",
        "        img_size=args.img_size,\n",
        "        text_vocab_size=args.text_vocab_size,\n",
        "        temperature_init=args.temperature_init,\n",
        "    )\n",
        "    return cfg  # ✅ now inside the function\n",
        "\n",
        "\n",
        "def main():\n",
        "    cfg = parse_args()\n",
        "    print(\"Config:\", cfg)\n",
        "    run_curriculum(cfg)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":  # ✅ correct double underscores\n",
        "    main()"
      ]
    }
  ]
}