{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNPjTt+DdFep2c/LPrVX5HF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Pinn/blob/main/MyAGIAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRDoGFrkEWcE"
      },
      "outputs": [],
      "source": [
        "# Single-file agent with a shared representation for regression and multimodal matching.\n",
        "# - Trainable PyTorch projections align text and image into a shared space.\n",
        "# - Regression uses a numeric -> shared -> scalar head.\n",
        "# - Grid/Symbol use lightweight heuristics to keep the harness runnable.\n",
        "# - Safe to drop into your harness; includes minimal fallbacks if base classes are missing.\n",
        "\n",
        "import os, time, random, math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "# Torch setup\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    TORCH_AVAILABLE = True\n",
        "except Exception:\n",
        "    TORCH_AVAILABLE = False\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "if TORCH_AVAILABLE:\n",
        "    torch.manual_seed(SEED)\n",
        "DEVICE = \"cuda\" if TORCH_AVAILABLE and torch.cuda.is_available() else \"cpu\"\n",
        "RUN_ID = f\"run_{int(time.time())}\"\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Harness base classes (only defined if they don't already exist)\n",
        "# ---------------------------------------------------------------------\n",
        "try:\n",
        "    AgentAdapter  # type: ignore[name-defined]\n",
        "except NameError:\n",
        "    @dataclass\n",
        "    class AgentConfig:\n",
        "        name: str = \"MyAGIAgent\"\n",
        "        use_world_model: bool = False\n",
        "        use_multimodal_encoder: bool = True\n",
        "        mc_dropout_passes: int = 0\n",
        "        reflection_steps: int = 1\n",
        "        notes: str = \"\"\n",
        "\n",
        "    class AgentAdapter:\n",
        "        def __init__(self, cfg: AgentConfig):\n",
        "            self.cfg = cfg\n",
        "        def act(self, obs: dict, task_id: str, step: int, state: dict | None):\n",
        "            raise NotImplementedError\n",
        "        def learn(self, batch: dict, task_id: str) -> dict:\n",
        "            return {\"learned\": False}\n",
        "        def reflect(self, logs: list[dict]) -> dict:\n",
        "            return {\"notes\": \"no-op\", \"patches\": {}}\n",
        "        def encode(self, modality: str, data):\n",
        "            raise NotImplementedError\n",
        "        def imagine(self, state: dict, n_steps: int = 5):\n",
        "            return []\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Core AGI agent with shared encoder\n",
        "# ---------------------------------------------------------------------\n",
        "class MyAGIAgent(nn.Module):\n",
        "    \"\"\"\n",
        "    Shared-representation agent:\n",
        "      - Text -> bag-of-chars (64) -> Linear -> shared_dim\n",
        "      - Image -> downsample 32x32 -> flatten(1024) -> Linear -> shared_dim\n",
        "      - Numeric (regression x) -> Linear -> shared_dim\n",
        "      - Heads: policy(shared->logits), regressor(shared->1)\n",
        "    \"\"\"\n",
        "    def __init__(self, shared_dim: int = 128, txt_dim: int = 64, img_hw: int = 32, eps: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.shared_dim = shared_dim\n",
        "        self.txt_dim = txt_dim\n",
        "        self.img_hw = img_hw\n",
        "        self.img_dim = img_hw * img_hw  # grayscale\n",
        "        self.eps = eps\n",
        "\n",
        "        # Shared MLP trunk (applied after modality-specific projection)\n",
        "        self.trunk = nn.Sequential(\n",
        "            nn.LayerNorm(shared_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(shared_dim, shared_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Modality projections\n",
        "        self.txt_proj = nn.Linear(txt_dim, shared_dim)\n",
        "        self.img_proj = nn.Linear(self.img_dim, shared_dim)\n",
        "        self.num_proj = nn.Linear(1, shared_dim)\n",
        "\n",
        "        # Heads\n",
        "        self.policy_head = nn.Linear(shared_dim, 4)  # up/down/left/right\n",
        "        self.reg_head = nn.Linear(shared_dim, 1)\n",
        "\n",
        "        # Temperature for contrastive similarity\n",
        "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(10.0)))  # ~e^2.3 â‰ˆ 10\n",
        "\n",
        "        # Simple optimizer sets (split so you can tune separately if desired)\n",
        "        self.reg_opt = torch.optim.Adam(list(self.num_proj.parameters()) + list(self.reg_head.parameters()) + list(self.trunk.parameters()), lr=5e-3)\n",
        "        self.mm_opt = torch.optim.Adam(list(self.txt_proj.parameters()) + list(self.img_proj.parameters()) + list(self.trunk.parameters()) + [self.logit_scale], lr=1e-3)\n",
        "\n",
        "        # Char vocab for text featurization\n",
        "        alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789_- \"\n",
        "        self.char2idx = {c: i + 1 for i, c in enumerate(alphabet)}  # 0 reserved\n",
        "\n",
        "    # ---- Featurizers ----\n",
        "    def featurize_text(self, s: str) -> torch.Tensor:\n",
        "        vec = np.zeros(self.txt_dim, dtype=np.float32)\n",
        "        for ch in s or \"\":\n",
        "            vec[self.char2idx.get(ch.lower(), 0) % self.txt_dim] += 1.0\n",
        "        vec = vec / (np.linalg.norm(vec) + 1e-8)\n",
        "        return torch.from_numpy(vec).to(DEVICE)\n",
        "\n",
        "    def featurize_image(self, img: np.ndarray) -> torch.Tensor:\n",
        "        # Expect HxWxC or HxW; downsample to img_hw x img_hw (grayscale) using torch interpolate\n",
        "        arr = torch.from_numpy(np.asarray(img)).float()\n",
        "        if arr.ndim == 2:\n",
        "            arr = arr.unsqueeze(0)  # 1,H,W\n",
        "        elif arr.ndim == 3:\n",
        "            if arr.shape[-1] in (1, 3):  # H,W,C -> C,H,W\n",
        "                arr = arr.permute(2, 0, 1)\n",
        "            else:\n",
        "                # Unknown last dim, collapse to 1 channel\n",
        "                arr = arr.mean(dim=-1, keepdim=True).permute(2, 0, 1)\n",
        "        else:\n",
        "            # Fallback: flatten and pad/trim later\n",
        "            flat = arr.flatten()\n",
        "            if flat.numel() < self.img_dim:\n",
        "                flat = F.pad(flat, (0, self.img_dim - flat.numel()))\n",
        "            else:\n",
        "                flat = flat[: self.img_dim]\n",
        "            flat = flat / (flat.norm() + 1e-8)\n",
        "            return flat.to(DEVICE)\n",
        "\n",
        "        arr = arr.unsqueeze(0)  # N=1,C,H,W\n",
        "        with torch.no_grad():\n",
        "            arr = F.interpolate(arr, size=(self.img_hw, self.img_hw), mode=\"bilinear\", align_corners=False)\n",
        "        gray = arr.mean(dim=1, keepdim=False).squeeze(0)  # H,W\n",
        "        vec = gray.flatten()\n",
        "        vec = vec / (vec.norm() + 1e-8)\n",
        "        return vec.to(DEVICE)\n",
        "\n",
        "    def featurize_num(self, x: np.ndarray) -> torch.Tensor:\n",
        "        # x: [B,1] numpy -> torch\n",
        "        t = torch.from_numpy(x).float().to(DEVICE)\n",
        "        return t\n",
        "\n",
        "    # ---- Encoders to shared space ----\n",
        "    def encode_text_shared(self, s: str) -> torch.Tensor:\n",
        "        z = self.txt_proj(self.featurize_text(s))\n",
        "        return self.trunk(z)\n",
        "\n",
        "    def encode_img_shared(self, img: np.ndarray) -> torch.Tensor:\n",
        "        z = self.img_proj(self.featurize_image(img))\n",
        "        return self.trunk(z)\n",
        "\n",
        "    def encode_num_shared(self, x_batch: np.ndarray) -> torch.Tensor:\n",
        "        z = self.num_proj(self.featurize_num(x_batch))  # [B, shared]\n",
        "        # Apply trunk per row\n",
        "        return self.trunk(z)\n",
        "\n",
        "    # ---- Heads ----\n",
        "    @torch.no_grad()\n",
        "    def act_regress(self, x_batch: np.ndarray) -> np.ndarray:\n",
        "        self.eval()\n",
        "        z = self.encode_num_shared(x_batch)  # [B, D]\n",
        "        y = self.reg_head(z)                 # [B, 1]\n",
        "        return y.cpu().numpy()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act_multimodal_txt2img(self, text: str, images: list[np.ndarray]) -> int:\n",
        "        self.eval()\n",
        "        tq = self.encode_text_shared(text)\n",
        "        sims = []\n",
        "        scale = self.logit_scale.exp().clamp(1.0, 100.0)\n",
        "        for img in images:\n",
        "            iv = self.encode_img_shared(img)\n",
        "            s = F.cosine_similarity(tq, iv, dim=0) * scale\n",
        "            sims.append(float(s.item()))\n",
        "        return int(np.argmax(sims))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act_multimodal_img2txt(self, image: np.ndarray, texts: list[str]) -> int:\n",
        "        self.eval()\n",
        "        iq = self.encode_img_shared(image)\n",
        "        sims = []\n",
        "        scale = self.logit_scale.exp().clamp(1.0, 100.0)\n",
        "        for t in texts:\n",
        "            tv = self.encode_text_shared(t)\n",
        "            s = F.cosine_similarity(iq, tv, dim=0) * scale\n",
        "            sims.append(float(s.item()))\n",
        "        return int(np.argmax(sims))\n",
        "\n",
        "    # ---- Learning steps ----\n",
        "    def step_regression(self, x: np.ndarray, y: np.ndarray) -> dict:\n",
        "        self.train()\n",
        "        z = self.encode_num_shared(x)  # [B,D]\n",
        "        pred = self.reg_head(z).squeeze(-1)  # [B]\n",
        "        target = torch.from_numpy(y.squeeze(-1)).float().to(DEVICE)\n",
        "        loss = F.mse_loss(pred, target)\n",
        "        self.reg_opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        self.reg_opt.step()\n",
        "        return {\"loss\": float(loss.item())}\n",
        "\n",
        "    def step_multimodal_txt2img(self, text: str, images: list[np.ndarray], target_idx: int) -> dict:\n",
        "        self.train()\n",
        "        tq = self.encode_text_shared(text)                 # [D]\n",
        "        ims = torch.stack([self.encode_img_shared(im) for im in images])  # [K,D]\n",
        "        scale = self.logit_scale.exp().clamp(1.0, 100.0)\n",
        "        logits = F.cosine_similarity(ims, tq.unsqueeze(0), dim=1) * scale  # [K]\n",
        "        target = torch.tensor([target_idx], device=DEVICE)\n",
        "        loss = F.cross_entropy(logits.unsqueeze(0), target)\n",
        "        self.mm_opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        self.mm_opt.step()\n",
        "        return {\"loss\": float(loss.item())}\n",
        "\n",
        "    def step_multimodal_img2txt(self, image: np.ndarray, texts: list[str], target_idx: int) -> dict:\n",
        "        self.train()\n",
        "        iq = self.encode_img_shared(image)                 # [D]\n",
        "        txs = torch.stack([self.encode_text_shared(t) for t in texts])  # [K,D]\n",
        "        scale = self.logit_scale.exp().clamp(1.0, 100.0)\n",
        "        logits = F.cosine_similarity(txs, iq.unsqueeze(0), dim=1) * scale  # [K]\n",
        "        target = torch.tensor([target_idx], device=DEVICE)\n",
        "        loss = F.cross_entropy(logits.unsqueeze(0), target)\n",
        "        self.mm_opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        self.mm_opt.step()\n",
        "        return {\"loss\": float(loss.item())}\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Adapter wiring to your harness interfaces\n",
        "# ---------------------------------------------------------------------\n",
        "class MyAGIAgentAdapter(AgentAdapter):\n",
        "    def __init__(self, cfg: \"AgentConfig\"):\n",
        "        super().__init__(cfg)\n",
        "        if not TORCH_AVAILABLE:\n",
        "            raise RuntimeError(\"This agent requires PyTorch.\")\n",
        "        self.agent = MyAGIAgent(shared_dim=128, txt_dim=64, img_hw=32, eps=0.1).to(DEVICE)\n",
        "        self.eps = 0.1  # used for grid heuristic exploration\n",
        "\n",
        "    def act(self, obs: dict, task_id: str, step: int, state: dict | None):\n",
        "        # Grid: choose from provided action_space, using a goal-directed heuristic with epsilon\n",
        "        if task_id.startswith(\"grid\"):\n",
        "            acts = obs[\"action_space\"]\n",
        "            if random.random() < self.eps:\n",
        "                a = random.choice(acts)\n",
        "            else:\n",
        "                pos = obs.get(\"pos\"); goal = obs.get(\"goal\")\n",
        "                def heuristic(act):\n",
        "                    dx, dy = {\"up\":(-1,0), \"down\":(1,0), \"left\":(0,-1), \"right\":(0,1)}.get(act,(0,0))\n",
        "                    nxt = (pos[0]+dx, pos[1]+dy)\n",
        "                    if goal is not None:\n",
        "                        return abs(nxt[0]-goal[0]) + abs(nxt[1]-goal[1])\n",
        "                    return 0.0\n",
        "                a = min(acts, key=heuristic)\n",
        "            return a, (state or {}), {}\n",
        "\n",
        "        # Symbol: pick rule that best reduces proxy distance to target (fallback random)\n",
        "        if task_id.startswith(\"symbol\"):\n",
        "            rules = obs[\"rules\"]; cur = obs[\"string\"]; tgt = obs[\"target\"]\n",
        "            best = None; best_score = math.inf\n",
        "            for (lhs, rhs) in rules:\n",
        "                idx = cur.find(lhs)\n",
        "                if idx >= 0:\n",
        "                    new_s = cur[:idx] + rhs + cur[idx+len(lhs):]\n",
        "                    score = abs(len(new_s)-len(tgt)) + sum(1 for a,b in zip(new_s, tgt) if a!=b)\n",
        "                    if score < best_score:\n",
        "                        best = (lhs, rhs); best_score = score\n",
        "            if best is None:\n",
        "                best = random.choice(rules) if rules else None\n",
        "            return best, (state or {}), {}\n",
        "\n",
        "        # Regression: obs[\"x\"] -> predictions [B,1] ndarray\n",
        "        if task_id.startswith(\"regress\"):\n",
        "            x = obs[\"x\"]  # np.array [B,1]\n",
        "            preds = self.agent.act_regress(x)\n",
        "            return preds, (state or {}), {}\n",
        "\n",
        "        # Multimodal matching: return plain int index\n",
        "        if task_id.startswith(\"multimodal\"):\n",
        "            mode = obs.get(\"mode\")\n",
        "            if mode == \"txt2img\":\n",
        "                idx = self.agent.act_multimodal_txt2img(obs[\"text\"], obs[\"images\"])\n",
        "                return int(idx), (state or {}), {}\n",
        "            else:\n",
        "                idx = self.agent.act_multimodal_img2txt(obs[\"image\"], obs[\"texts\"])\n",
        "                return int(idx), (state or {}), {}\n",
        "\n",
        "        return None, (state or {}), {}\n",
        "\n",
        "    def learn(self, batch: dict, task_id: str) -> dict:\n",
        "        if task_id.startswith(\"regress\"):\n",
        "            return self.agent.step_regression(batch[\"x\"], batch[\"y\"])\n",
        "        if task_id.startswith(\"multimodal\"):\n",
        "            mode = batch.get(\"mode\")\n",
        "            if mode == \"txt2img\":\n",
        "                return self.agent.step_multimodal_txt2img(batch[\"text\"], batch[\"images\"], batch[\"target_idx\"])\n",
        "            else:\n",
        "                return self.agent.step_multimodal_img2txt(batch[\"image\"], batch[\"texts\"], batch[\"target_idx\"])\n",
        "        return {\"loss\": None}\n",
        "\n",
        "    def reflect(self, logs: list[dict]) -> dict:\n",
        "        # Adjust exploration based on recent failures\n",
        "        fails = [1.0 if r.get(\"success\")==0 else 0.0 for r in logs if \"success\" in r]\n",
        "        fail_rate = float(np.mean(fails)) if fails else 0.0\n",
        "        if fail_rate > 0.5:\n",
        "            self.eps = min(0.3, self.eps + 0.05)\n",
        "            note = f\"Increased epsilon to {self.eps:.2f} after fail_rate={fail_rate:.2f}\"\n",
        "        else:\n",
        "            note = f\"No change; fail_rate={fail_rate:.2f}\"\n",
        "        return {\"notes\": note, \"patches\": {\"eps\": self.eps}}\n",
        "\n",
        "    def encode(self, modality: str, data):\n",
        "        if modality == \"text\":\n",
        "            with torch.no_grad():\n",
        "                v = self.agent.encode_text_shared(data)\n",
        "                return v.detach().cpu().numpy()\n",
        "        if modality == \"image\":\n",
        "            with torch.no_grad():\n",
        "                v = self.agent.encode_img_shared(data)\n",
        "                return v.detach().cpu().numpy()\n",
        "        raise ValueError(f\"Unknown modality: {modality}\")\n",
        "\n",
        "    def imagine(self, state: dict, n_steps: int = 5):\n",
        "        # Placeholder: no world model yet\n",
        "        return []\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Optional: run if your harness is available; otherwise just define agent.\n",
        "# ---------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    if \"run_all\" in globals():\n",
        "        cfg = AgentConfig(\n",
        "            name=\"MyAGIAgent\",\n",
        "            use_world_model=False,\n",
        "            use_multimodal_encoder=True,\n",
        "            notes=\"shared-encoder, trainable multimodal projections\"\n",
        "        )\n",
        "        agent = MyAGIAgentAdapter(cfg)\n",
        "        out = run_all(agent)  # expected to return (df, refl_notes, sym_test, grid_test, sin_test)\n",
        "        try:\n",
        "            df = out[0]\n",
        "            dashboard(df)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"Run complete:\", RUN_ID)\n",
        "    else:\n",
        "        print(\"Defined MyAGIAgentAdapter. Integrate with your harness via: df, refl_notes, sym_test, grid_test, sin_test = run_all(MyAGIAgentAdapter(AgentConfig(...))).\")"
      ]
    }
  ]
}