{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPLa8mVlb02pgKl6myV/1uz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Pinn/blob/main/agi_self_reflect_cartpole_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ5OvWmcUMUT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "AGIAgent: environment-ready, memory-augmented, self-reflective RL on CartPole-v1\n",
        "\n",
        "What this does\n",
        "- Policy + value networks collect episodes in CartPole.\n",
        "- Experiences are encoded and written into a simple differentiable memory.\n",
        "- A lightweight \"reasoner\" attends over memory for queries.\n",
        "- A planner wrapper selects actions from the policy (extensible to lookahead).\n",
        "- After each batch, the agent \"reflects\" by replaying memory to refine the critic,\n",
        "  then updates the policy with refined advantages.\n",
        "\n",
        "Safe defaults\n",
        "- Seeding, gradient clipping, entropy bonus, normalized advantages, checkpoints.\n",
        "\n",
        "Run\n",
        "  python agi_self_reflect_cartpole.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Gym import with fallback\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "except ImportError:\n",
        "    import gym\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Config and utilities\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    env_id: str = \"CartPole-v1\"\n",
        "    max_episodes: int = 400\n",
        "    episodes_per_batch: int = 8\n",
        "    gamma: float = 0.99\n",
        "    policy_lr: float = 3e-4\n",
        "    critic_lr: float = 5e-4\n",
        "    entropy_coef: float = 0.01\n",
        "    value_coef: float = 0.5\n",
        "    grad_clip: float = 0.5\n",
        "    reflection_epochs_critic: int = 4\n",
        "    reflection_epochs_policy: int = 2\n",
        "    hidden_sizes: tuple = (128, 128)\n",
        "    seed: int = 42\n",
        "    device: str = \"cpu\"  # set to \"cuda\" if available\n",
        "    log_interval: int = 10\n",
        "    checkpoint_every: int = 100\n",
        "    out_dir: str = \"checkpoints_agi\"\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def to_tensor(x, device):\n",
        "    return torch.as_tensor(x, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Models and encoders\n",
        "# -----------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_sizes=(128, 128), act=nn.Tanh):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers += [nn.Linear(last, h), act()]\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, out_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions, hidden_sizes=(128, 128)):\n",
        "        super().__init__()\n",
        "        self.backbone = MLP(obs_dim, n_actions, hidden_sizes=hidden_sizes, act=nn.Tanh)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return self.backbone(obs)  # logits\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_sizes=(128, 128)):\n",
        "        super().__init__()\n",
        "        self.backbone = MLP(obs_dim, 1, hidden_sizes=hidden_sizes, act=nn.Tanh)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return self.backbone(obs).squeeze(-1)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim, emb_dim=64, hidden_sizes=(64,)):\n",
        "        super().__init__()\n",
        "        self.proj = MLP(in_dim, emb_dim, hidden_sizes=hidden_sizes, act=nn.ReLU)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Simple differentiable memory\n",
        "# -----------------------------\n",
        "\n",
        "class DifferentiableMemory(nn.Module):\n",
        "    \"\"\"\n",
        "    Content-addressable memory:\n",
        "    - write: encode experience dict -> embedding, append to memory bank\n",
        "    - read: cosine attention over embeddings with a query embedding\n",
        "    Memory embeddings are differentiable via the encoder; retrieval uses soft weights.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder: Encoder, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.device = torch.device(device)\n",
        "        self.clear()\n",
        "\n",
        "    def clear(self):\n",
        "        self.embeds = []      # list of tensors [E]\n",
        "        self.payloads = []    # raw experiences\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def write(self, experience: dict):\n",
        "        # Expect experience has 'state' (tensor-like) and optional extras\n",
        "        state = experience.get(\"state\")\n",
        "        if not torch.is_tensor(state):\n",
        "            state = to_tensor(state, self.device)\n",
        "        emb = self.encoder(state.unsqueeze(0)).squeeze(0).detach()\n",
        "        self.embeds.append(emb)\n",
        "        self.payloads.append(experience)\n",
        "\n",
        "    def read(self, query_embedding: torch.Tensor, topk=3):\n",
        "        if len(self.embeds) == 0:\n",
        "            return [], torch.empty(0, device=self.device)\n",
        "        E = torch.stack(self.embeds, dim=0)  # [N, D]\n",
        "        q = query_embedding.unsqueeze(0)     # [1, D]\n",
        "        # cosine similarity\n",
        "        E_norm = torch.nn.functional.normalize(E, dim=-1)\n",
        "        q_norm = torch.nn.functional.normalize(q, dim=-1)\n",
        "        sims = (E_norm @ q_norm.t()).squeeze(-1)  # [N]\n",
        "        vals, idx = torch.topk(sims, k=min(topk, E.shape[0]))\n",
        "        results = [self.payloads[i] for i in idx.tolist()]\n",
        "        return results, vals\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Reasoner and planner\n",
        "# -----------------------------\n",
        "\n",
        "class SimpleReasoner:\n",
        "    \"\"\"\n",
        "    Aggregates top-k memory matches and returns a small report.\n",
        "    Extendable to structured logic or graph query.\n",
        "    \"\"\"\n",
        "    def __init__(self, memory: DifferentiableMemory):\n",
        "        self.memory = memory\n",
        "\n",
        "    def __call__(self, query_embedding, topk=3):\n",
        "        items, weights = self.memory.read(query_embedding, topk=topk)\n",
        "        summary = {\n",
        "            \"count\": len(items),\n",
        "            \"weights\": weights.detach().cpu().tolist(),\n",
        "            \"samples\": [{k: (v if isinstance(v, (int, float, str)) else \"tensor\")}\n",
        "                        for k, v in (items[0].items() if items else [])]\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "class PolicyPlanner:\n",
        "    \"\"\"\n",
        "    Planner wrapper: selects actions via policy distribution, with temperature.\n",
        "    You can replace this with MCTS/model-based lookahead later.\n",
        "    \"\"\"\n",
        "    def __init__(self, policy: PolicyNet, temperature: float = 1.0, device=\"cpu\"):\n",
        "        self.policy = policy\n",
        "        self.temperature = max(1e-3, float(temperature))\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, state_tensor: torch.Tensor):\n",
        "        logits = self.policy(state_tensor.unsqueeze(0)) / self.temperature\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample().item()\n",
        "        return action\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Self-reflective RL core\n",
        "# -----------------------------\n",
        "\n",
        "class TrajectoryBuffer:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.clear()\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logps = []\n",
        "        self.values = []\n",
        "        self.episode_returns = []\n",
        "\n",
        "    def store_step(self, state, action, reward, done, logp, value):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.logps.append(logp)\n",
        "        self.values.append(value)\n",
        "\n",
        "    def finalize_episode(self, ep_return):\n",
        "        self.episode_returns.append(ep_return)\n",
        "\n",
        "    def as_tensors(self):\n",
        "        states = torch.stack(self.states, dim=0).to(self.device)\n",
        "        actions = torch.as_tensor(self.actions, dtype=torch.int64, device=self.device)\n",
        "        rewards = torch.as_tensor(self.rewards, dtype=torch.float32, device=self.device)\n",
        "        dones = torch.as_tensor(self.dones, dtype=torch.bool, device=self.device)\n",
        "        logps = torch.stack(self.logps, dim=0).to(self.device)\n",
        "        values = torch.as_tensor(self.values, dtype=torch.float32, device=self.device)\n",
        "        return states, actions, rewards, dones, logps, values\n",
        "\n",
        "class SelfReflectiveRL:\n",
        "    def __init__(self, obs_dim, n_actions, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "        self.device = torch.device(cfg.device)\n",
        "        self.policy = PolicyNet(obs_dim, n_actions, cfg.hidden_sizes).to(self.device)\n",
        "        self.critic = ValueNet(obs_dim, cfg.hidden_sizes).to(self.device)\n",
        "        self.pi_opt = optim.Adam(self.policy.parameters(), lr=cfg.policy_lr)\n",
        "        self.vf_opt = optim.Adam(self.critic.parameters(), lr=cfg.critic_lr)\n",
        "        self.buffer = TrajectoryBuffer(self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act(self, obs):\n",
        "        obs_t = to_tensor(obs, self.device).unsqueeze(0)\n",
        "        logits = self.policy(obs_t)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        logp = dist.log_prob(action).squeeze(0)\n",
        "        value = self.critic(obs_t).squeeze(0)\n",
        "        return int(action.item()), logp, float(value.item())\n",
        "\n",
        "    def compute_returns_and_advantages(self, rewards, dones, values, gamma):\n",
        "        returns = torch.zeros_like(rewards, device=self.device)\n",
        "        G = 0.0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if dones[t]:\n",
        "                G = 0.0\n",
        "            G = rewards[t] + gamma * G\n",
        "            returns[t] = G\n",
        "        advantages = returns - values.detach()\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std(unbiased=False) + 1e-8)\n",
        "        return returns, advantages\n",
        "\n",
        "    def reflect_and_update(self):\n",
        "        states, actions, rewards, dones, logps_old, values = self.buffer.as_tensors()\n",
        "        returns, advantages = self.compute_returns_and_advantages(rewards, dones, values, self.cfg.gamma)\n",
        "\n",
        "        # Critic reflection\n",
        "        for _ in range(self.cfg.reflection_epochs_critic):\n",
        "            v_pred = self.critic(states)\n",
        "            v_loss = torch.nn.functional.mse_loss(v_pred, returns)\n",
        "            self.vf_opt.zero_grad()\n",
        "            v_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.grad_clip)\n",
        "            self.vf_opt.step()\n",
        "\n",
        "        # Recompute with refined critic\n",
        "        with torch.no_grad():\n",
        "            values_refined = self.critic(states)\n",
        "        returns_refined, advantages_refined = self.compute_returns_and_advantages(\n",
        "            rewards, dones, values_refined, self.cfg.gamma\n",
        "        )\n",
        "\n",
        "        # Policy reflection (+ small auxiliary value fit and entropy)\n",
        "        for _ in range(self.cfg.reflection_epochs_policy):\n",
        "            logits = self.policy(states)\n",
        "            dist = torch.distributions.Categorical(logits=logits)\n",
        "            logps = dist.log_prob(actions)\n",
        "            entropy = dist.entropy().mean()\n",
        "\n",
        "            policy_loss = -(logps * advantages_refined).mean()\n",
        "            value_loss = torch.nn.functional.mse_loss(self.critic(states), returns_refined)\n",
        "            loss = policy_loss + self.cfg.value_coef * value_loss - self.cfg.entropy_coef * entropy\n",
        "\n",
        "            self.pi_opt.zero_grad()\n",
        "            self.vf_opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.cfg.grad_clip)\n",
        "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.grad_clip)\n",
        "            self.pi_opt.step()\n",
        "            self.vf_opt.step()\n",
        "\n",
        "        stats = {\n",
        "            \"batch_ep_mean\": float(np.mean(self.buffer.episode_returns)) if self.buffer.episode_returns else float(\"nan\"),\n",
        "            \"batch_ep_count\": len(self.buffer.episode_returns),\n",
        "        }\n",
        "        self.buffer.clear()\n",
        "        return stats\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Optional: lightweight MAML stub\n",
        "# -----------------------------\n",
        "\n",
        "class MAMLStub:\n",
        "    \"\"\"\n",
        "    Placeholder for meta-learner. Returns a small supervised loss on embeddings.\n",
        "    You can swap this with your real MAML implementation without changing AGIAgent.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder: Encoder, lr_inner=0.01):\n",
        "        self.encoder = encoder\n",
        "        self.lr_inner = lr_inner\n",
        "\n",
        "    def __call__(self, task_batch: torch.Tensor):\n",
        "        emb = self.encoder(task_batch)\n",
        "        target = torch.zeros_like(emb)\n",
        "        return ((emb - target) ** 2).mean()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# AGIAgent orchestration\n",
        "# -----------------------------\n",
        "\n",
        "class AGIAgent:\n",
        "    def __init__(self, obs_dim, n_actions, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "        self.device = torch.device(cfg.device)\n",
        "\n",
        "        # Core RL\n",
        "        self.rl = SelfReflectiveRL(obs_dim, n_actions, cfg)\n",
        "\n",
        "        # Shared encoder for memory/reasoning (decoupled from policy/value)\n",
        "        self.encoder = Encoder(in_dim=obs_dim, emb_dim=64, hidden_sizes=(64,)).to(self.device)\n",
        "\n",
        "        # Memory + reasoner + planner\n",
        "        self.memory = DifferentiableMemory(self.encoder, device=self.device)\n",
        "        self.reasoner = SimpleReasoner(self.memory)\n",
        "        self.planner = PolicyPlanner(self.rl.policy, temperature=1.0, device=self.device)\n",
        "\n",
        "        # Optional meta-learner on encoded states (stub)\n",
        "        self.maml = MAMLStub(self.encoder, lr_inner=0.01)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x_t = x if torch.is_tensor(x) else to_tensor(x, self.device)\n",
        "        return self.encoder(x_t)\n",
        "\n",
        "    def store_experience(self, experience: dict):\n",
        "        self.memory.write(experience)\n",
        "\n",
        "    def reason_about(self, query_vec):\n",
        "        if not torch.is_tensor(query_vec):\n",
        "            query_vec = to_tensor(query_vec, self.device)\n",
        "        if query_vec.ndim == 1:\n",
        "            q_emb = self.encoder(query_vec)\n",
        "        else:\n",
        "            q_emb = self.encoder(query_vec).mean(dim=0)\n",
        "        return self.reasoner(q_emb, topk=3)\n",
        "\n",
        "    def plan_action(self, state_tensor: torch.Tensor):\n",
        "        return self.planner(state_tensor)\n",
        "\n",
        "    def learn_meta(self, task_batch: torch.Tensor):\n",
        "        loss = self.maml(task_batch)\n",
        "        loss.backward()\n",
        "        return float(loss.item())\n",
        "\n",
        "    def interact_and_learn(self, env, max_episodes=None):\n",
        "        cfg = self.cfg\n",
        "        max_episodes = max_episodes or cfg.max_episodes\n",
        "\n",
        "        ep_count = 0\n",
        "        rolling = []\n",
        "        best_avg = -math.inf\n",
        "        os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "        while ep_count < max_episodes:\n",
        "            # Collect a batch of episodes\n",
        "            batch_returns = []\n",
        "            batch_start = ep_count\n",
        "\n",
        "            while ep_count - batch_start < cfg.episodes_per_batch and ep_count < max_episodes:\n",
        "                obs, info = env.reset(seed=cfg.seed + ep_count) if hasattr(env, \"reset\") and \"gymnasium\" in env.__class__.__module__ else (env.reset(), {})\n",
        "                done = False\n",
        "                ep_return = 0.0\n",
        "\n",
        "                while not done:\n",
        "                    state_t = to_tensor(obs, self.device)\n",
        "                    # Plan (wrapper over policy)\n",
        "                    action = self.plan_action(state_t)\n",
        "\n",
        "                    # Step environment\n",
        "                    step_out = env.step(action)\n",
        "                    if len(step_out) == 5:\n",
        "                        next_obs, reward, terminated, truncated, info = step_out\n",
        "                        done = terminated or truncated\n",
        "                    else:\n",
        "                        next_obs, reward, done, info = step_out\n",
        "\n",
        "                    # Also query RL policy for logp/value for learning\n",
        "                    a_rl, logp, value = self.rl.act(obs)  # note: a_rl is sampled; we record action we actually took\n",
        "                    # Store into RL buffer (with the action we executed via planner)\n",
        "                    self.rl.buffer.store_step(\n",
        "                        state=state_t,\n",
        "                        action=action,\n",
        "                        reward=reward,\n",
        "                        done=done,\n",
        "                        logp=logp,\n",
        "                        value=value,\n",
        "                    )\n",
        "\n",
        "                    # Write to external memory\n",
        "                    self.store_experience({\n",
        "                        \"state\": state_t.detach().cpu(),\n",
        "                        \"action\": int(action),\n",
        "                        \"reward\": float(reward),\n",
        "                        \"done\": bool(done),\n",
        "                    })\n",
        "\n",
        "                    obs = next_obs\n",
        "                    ep_return += reward\n",
        "\n",
        "                self.rl.buffer.finalize_episode(ep_return)\n",
        "                batch_returns.append(ep_return)\n",
        "                rolling.append(ep_return)\n",
        "                if len(rolling) > 50:\n",
        "                    rolling.pop(0)\n",
        "                ep_count += 1\n",
        "\n",
        "            # Reflection and updates over the collected batch\n",
        "            reflect_stats = self.rl.reflect_and_update()\n",
        "\n",
        "            # Logging\n",
        "            avg_return = float(np.mean(rolling)) if rolling else 0.0\n",
        "            last_batch_mean = float(np.mean(batch_returns)) if batch_returns else float(\"nan\")\n",
        "            if ep_count % cfg.log_interval == 0:\n",
        "                print(f\"[{ep_count:4d}] avg_return(50)={avg_return:7.2f}  last_batch_mean={last_batch_mean:7.2f}  batch_ep_count={reflect_stats['batch_ep_count']}\")\n",
        "\n",
        "            # Checkpointing\n",
        "            if ep_count % cfg.checkpoint_every == 0:\n",
        "                torch.save(self.rl.policy.state_dict(), os.path.join(cfg.out_dir, f\"ep{ep_count}_policy.pt\"))\n",
        "                torch.save(self.rl.critic.state_dict(), os.path.join(cfg.out_dir, f\"ep{ep_count}_critic.pt\"))\n",
        "                if avg_return > best_avg:\n",
        "                    best_avg = avg_return\n",
        "                    torch.save(self.rl.policy.state_dict(), os.path.join(cfg.out_dir, \"best_policy.pt\"))\n",
        "                    torch.save(self.rl.critic.state_dict(), os.path.join(cfg.out_dir, \"best_critic.pt\"))\n",
        "\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Entrypoint\n",
        "# -----------------------------\n",
        "\n",
        "def main():\n",
        "    cfg = Config()\n",
        "    if torch.cuda.is_available():\n",
        "        cfg.device = \"cuda\"\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    env = gym.make(cfg.env_id)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    agent = AGIAgent(obs_dim, n_actions, cfg)\n",
        "\n",
        "    # Optional: quick meta-learning warmup on random \"tasks\"\n",
        "    task_batch = torch.randn(16, obs_dim, device=cfg.device)\n",
        "    _ = agent.learn_meta(task_batch)\n",
        "\n",
        "    # Train in the environment with self-reflection and memory\n",
        "    agent.interact_and_learn(env, max_episodes=cfg.max_episodes)\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}