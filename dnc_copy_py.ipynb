{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMcgeZ2gw78mrfLl1nhD0yG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Pinn/blob/main/dnc_copy_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hspAApjJIQ9p"
      },
      "outputs": [],
      "source": [
        "# dnc_copy.py\n",
        "# Trainable Differentiable Neural Computer (DNC) with:\n",
        "# - Content-based read/write\n",
        "# - Usage-based allocation\n",
        "# - Temporal link matrix (forward/backward traversal)\n",
        "# Includes a copy task trainer for verification.\n",
        "\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "from typing import Tuple, NamedTuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "\n",
        "def set_seed(seed: int = 42, deterministic: bool = True):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def softplus(x, beta=1.0, threshold=20.0):\n",
        "    return F.softplus(x, beta=beta, threshold=threshold)\n",
        "\n",
        "\n",
        "def cosine_similarity(M: torch.Tensor, k: torch.Tensor, eps: float = 1e-8):\n",
        "    \"\"\"\n",
        "    M: (B, N, W) memory\n",
        "    k: (B, H, W) keys (H can be 1 or num_heads)\n",
        "    returns: (B, H, N)\n",
        "    \"\"\"\n",
        "    B, N, W = M.shape\n",
        "    H = k.size(1)\n",
        "    Mk = (M / (M.norm(dim=2, keepdim=True) + eps))  # (B, N, W)\n",
        "    kk = (k / (k.norm(dim=2, keepdim=True) + eps))  # (B, H, W)\n",
        "    # (B, H, N) = (B, H, W) @ (B, W, N)\n",
        "    return torch.matmul(kk, Mk.transpose(1, 2))\n",
        "\n",
        "\n",
        "def batched_outer(a: torch.Tensor, b: torch.Tensor):\n",
        "    \"\"\"\n",
        "    a: (B, N) ; b: (B, N)\n",
        "    returns: (B, N, N) with outer products per batch\n",
        "    \"\"\"\n",
        "    return a.unsqueeze(2) * b.unsqueeze(1)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# DNC core\n",
        "# ---------------------------\n",
        "\n",
        "class DNCState(NamedTuple):\n",
        "    controller_h: torch.Tensor       # (L, B, H)\n",
        "    controller_c: torch.Tensor       # (L, B, H)\n",
        "    memory: torch.Tensor             # (B, N, W)\n",
        "    usage: torch.Tensor              # (B, N)\n",
        "    link: torch.Tensor               # (B, N, N)\n",
        "    precedence: torch.Tensor         # (B, N)\n",
        "    read_weights: torch.Tensor       # (B, R, N)\n",
        "    read_vectors: torch.Tensor       # (B, R, W)\n",
        "    write_weights: torch.Tensor      # (B, N)\n",
        "\n",
        "\n",
        "class DNC(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        output_size: int,\n",
        "        controller_hidden: int = 256,\n",
        "        controller_layers: int = 1,\n",
        "        mem_n: int = 64,\n",
        "        mem_w: int = 32,\n",
        "        read_heads: int = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.H = controller_hidden\n",
        "        self.L = controller_layers\n",
        "        self.N = mem_n\n",
        "        self.W = mem_w\n",
        "        self.R = read_heads\n",
        "\n",
        "        # Controller sees input plus previous read vectors\n",
        "        ctrl_in = input_size + read_heads * mem_w\n",
        "        self.controller = nn.LSTM(ctrl_in, controller_hidden, num_layers=controller_layers)\n",
        "\n",
        "        # Parameter head from controller to interface vector\n",
        "        # Write: k_w(W), beta_w(1), e(W), v(W), g_a(1), g_w(1)\n",
        "        # Read:  k_r(R*W), beta_r(R), f(R), pi(R*3)\n",
        "        interface_size = (3 * self.W + 2) + self.R * (self.W + 1 + 1 + 3)\n",
        "        self.interface_linear = nn.Linear(controller_hidden, interface_size)\n",
        "\n",
        "        # Output layer sees controller output + read vectors\n",
        "        self.output_layer = nn.Linear(controller_hidden + read_heads * mem_w, output_size)\n",
        "\n",
        "        # Initialization helpers\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for name, p in self.named_parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "        # Slightly bias strengths positive\n",
        "        # No special-case biasing here; stability is handled by activations\n",
        "\n",
        "    # -------- Addressing subroutines --------\n",
        "\n",
        "    def _content_address(self, M, k, beta):\n",
        "        \"\"\"\n",
        "        M: (B, N, W)\n",
        "        k: (B, H, W)\n",
        "        beta: (B, H, 1) >= 0\n",
        "        returns: (B, H, N)\n",
        "        \"\"\"\n",
        "        sim = cosine_similarity(M, k)  # (B, H, N)\n",
        "        return F.softmax(beta * sim, dim=-1)\n",
        "\n",
        "    def _update_usage(self, usage, read_weights, write_weights, free_gates):\n",
        "        \"\"\"\n",
        "        usage: (B, N)\n",
        "        read_weights: (B, R, N)\n",
        "        write_weights: (B, N)\n",
        "        free_gates: (B, R, 1) in [0,1]\n",
        "        returns: (B, N)\n",
        "        \"\"\"\n",
        "        # After reads with free gates\n",
        "        psi = torch.prod(1 - free_gates * read_weights.unsqueeze(2), dim=1).squeeze(-1)  # (B, N)\n",
        "        usage = (usage + write_weights - usage * write_weights)  # u + w - uâˆ˜w\n",
        "        usage = usage * psi\n",
        "        return torch.clamp(usage, 0.0, 1.0)\n",
        "\n",
        "    def _allocation_weighting(self, usage):\n",
        "        \"\"\"\n",
        "        usage: (B, N) in [0,1]\n",
        "        returns allocation weights a: (B, N), favoring least used slots.\n",
        "        \"\"\"\n",
        "        B, N = usage.shape\n",
        "        # sort ascending by usage\n",
        "        sorted_usage, indices = torch.sort(usage, dim=1)\n",
        "        # cumulative product of sorted usage\n",
        "        cumprod = torch.cumprod(sorted_usage, dim=1)\n",
        "        # a_j = (1 - u_j) * prod_{i<j} u_i; with prod_{i<0}=1\n",
        "        # Shift cumprod right and insert ones at start\n",
        "        one = torch.ones(B, 1, device=usage.device, dtype=usage.dtype)\n",
        "        prod_prev = torch.cat([one, cumprod[:, :-1]], dim=1)\n",
        "        a_sorted = (1 - sorted_usage) * prod_prev\n",
        "        # unsort back\n",
        "        a = torch.zeros_like(a_sorted).scatter(1, indices, a_sorted)\n",
        "        # normalize just in case of zeros everywhere\n",
        "        denom = a.sum(dim=1, keepdim=True) + 1e-8\n",
        "        return a / denom\n",
        "\n",
        "    def _write(self, M, usage, link, precedence, read_weights, interface):\n",
        "        \"\"\"\n",
        "        M: (B, N, W)\n",
        "        usage: (B, N)\n",
        "        link: (B, N, N)\n",
        "        precedence: (B, N)\n",
        "        read_weights: (B, R, N)\n",
        "        interface: dict of write params\n",
        "        returns updated (M, usage, link, precedence, write_weights)\n",
        "        \"\"\"\n",
        "        B, N, W = M.shape\n",
        "        # Unpack write interface\n",
        "        k_w = interface[\"k_w\"]               # (B, 1, W)\n",
        "        beta_w = interface[\"beta_w\"]         # (B, 1, 1)\n",
        "        e = interface[\"e\"]                   # (B, 1, W) in [0,1]\n",
        "        v = interface[\"v\"]                   # (B, 1, W)\n",
        "        g_a = interface[\"g_a\"]               # (B, 1, 1) in [0,1]\n",
        "        g_w = interface[\"g_w\"]               # (B, 1, 1) in [0,1]\n",
        "        free_gates = interface[\"free_gates\"] # (B, R, 1)\n",
        "\n",
        "        # Update usage with current read weights and previous usage (before new write)\n",
        "        usage = self._update_usage(usage, read_weights, torch.zeros(B, N, device=M.device), free_gates)\n",
        "\n",
        "        # Content weights for write key\n",
        "        c_w = self._content_address(M, k_w, beta_w).squeeze(1)  # (B, N)\n",
        "\n",
        "        # Allocation weights from updated usage\n",
        "        a = self._allocation_weighting(usage)                   # (B, N)\n",
        "\n",
        "        # Interpolate allocation vs content, then gate by write gate\n",
        "        w_w = g_w.squeeze(-1) * (g_a.squeeze(-1) * a + (1 - g_a.squeeze(-1)) * c_w)  # (B, N)\n",
        "\n",
        "        # Erase and add\n",
        "        # M = M * (1 - w_w e^T) + w_w v^T\n",
        "        erase_term = 1 - w_w.unsqueeze(-1) * e  # (B, N, W)\n",
        "        add_term = w_w.unsqueeze(-1) * v        # (B, N, W)\n",
        "        M = M * erase_term + add_term\n",
        "\n",
        "        # Update usage after the write\n",
        "        usage = self._update_usage(usage, read_weights, w_w, free_gates)\n",
        "\n",
        "        # Update temporal links\n",
        "        w_col = w_w.unsqueeze(2)  # (B, N, 1)\n",
        "        w_row = w_w.unsqueeze(1)  # (B, 1, N)\n",
        "\n",
        "        link = (1 - torch.eye(N, device=M.device).unsqueeze(0)) * link  # zero diag\n",
        "        link = link + batched_outer(w_w, precedence)  # add new links\n",
        "        link = link * (1 - torch.eye(N, device=M.device).unsqueeze(0))  # ensure zero diag\n",
        "\n",
        "        precedence = (1 - w_w.sum(dim=1, keepdim=True)) * precedence + w_w  # (B, N)\n",
        "\n",
        "        return M, usage, link, precedence, w_w\n",
        "\n",
        "    def _read(self, M, link, read_weights_prev, interface):\n",
        "        \"\"\"\n",
        "        M: (B, N, W)\n",
        "        link: (B, N, N)\n",
        "        read_weights_prev: (B, R, N)\n",
        "        interface: dict with read keys, strengths, modes\n",
        "        returns (read_weights, read_vectors)\n",
        "        \"\"\"\n",
        "        B, N, W = M.shape\n",
        "\n",
        "        k_r = interface[\"k_r\"]           # (B, R, W)\n",
        "        beta_r = interface[\"beta_r\"]     # (B, R, 1)\n",
        "        read_modes = interface[\"read_modes\"]  # (B, R, 3) rows sum to 1\n",
        "\n",
        "        # Content weights\n",
        "        c_r = self._content_address(M, k_r, beta_r)             # (B, R, N)\n",
        "\n",
        "        # Forward and backward traversals via link matrix\n",
        "        fwd = torch.matmul(read_weights_prev, link)              # (B, R, N)\n",
        "        bwd = torch.matmul(read_weights_prev, link.transpose(1, 2))  # (B, R, N)\n",
        "\n",
        "        # Combine by read modes [backward, content, forward]\n",
        "        pi_b = read_modes[..., 0:1]\n",
        "        pi_c = read_modes[..., 1:2]\n",
        "        pi_f = read_modes[..., 2:3]\n",
        "        read_weights = pi_b * bwd + pi_c * c_r + pi_f * fwd     # (B, R, N)\n",
        "        read_weights = read_weights + 1e-8\n",
        "        read_weights = read_weights / read_weights.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        read_vectors = torch.matmul(read_weights, M)            # (B, R, W)\n",
        "\n",
        "        return read_weights, read_vectors\n",
        "\n",
        "    # -------- Parsing interface vector --------\n",
        "\n",
        "    def _parse_interface(self, iface: torch.Tensor, B: int):\n",
        "        \"\"\"\n",
        "        iface: (T, B, interface_size) or (B, interface_size) for single step\n",
        "        We handle single step here (B, D).\n",
        "        Returns dict of shaped tensors.\n",
        "        \"\"\"\n",
        "        D = iface.size(-1)\n",
        "        ofs = 0\n",
        "\n",
        "        def take(n):\n",
        "            nonlocal ofs\n",
        "            out = iface[:, ofs:ofs+n]\n",
        "            ofs += n\n",
        "            return out\n",
        "\n",
        "        # Write\n",
        "        k_w = take(self.W).view(B, 1, self.W)\n",
        "        beta_w = softplus(take(1)).view(B, 1, 1) + 1e-6\n",
        "        e = torch.sigmoid(take(self.W)).view(B, 1, self.W)\n",
        "        v = torch.tanh(take(self.W)).view(B, 1, self.W)\n",
        "        g_a = torch.sigmoid(take(1)).view(B, 1, 1)\n",
        "        g_w = torch.sigmoid(take(1)).view(B, 1, 1)\n",
        "\n",
        "        # Read\n",
        "        k_r = take(self.R * self.W).view(B, self.R, self.W)\n",
        "        beta_r = softplus(take(self.R)).view(B, self.R, 1) + 1e-6\n",
        "        free_gates = torch.sigmoid(take(self.R)).view(B, self.R, 1)\n",
        "        read_modes = take(self.R * 3).view(B, self.R, 3)\n",
        "        read_modes = F.softmax(read_modes, dim=-1)\n",
        "\n",
        "        return {\n",
        "            \"k_w\": k_w,\n",
        "            \"beta_w\": beta_w,\n",
        "            \"e\": e,\n",
        "            \"v\": v,\n",
        "            \"g_a\": g_a,\n",
        "            \"g_w\": g_w,\n",
        "            \"k_r\": k_r,\n",
        "            \"beta_r\": beta_r,\n",
        "            \"free_gates\": free_gates,\n",
        "            \"read_modes\": read_modes,\n",
        "        }\n",
        "\n",
        "    # -------- Step and forward --------\n",
        "\n",
        "    def initial_state(self, batch_size: int, device: str):\n",
        "        controller_h = torch.zeros(self.L, batch_size, self.H, device=device)\n",
        "        controller_c = torch.zeros(self.L, batch_size, self.H, device=device)\n",
        "        memory = torch.zeros(batch_size, self.N, self.W, device=device)\n",
        "        usage = torch.zeros(batch_size, self.N, device=device)\n",
        "        link = torch.zeros(batch_size, self.N, self.N, device=device)\n",
        "        precedence = torch.zeros(batch_size, self.N, device=device)\n",
        "        read_weights = F.softmax(torch.zeros(batch_size, self.R, self.N, device=device), dim=-1)\n",
        "        read_vectors = torch.zeros(batch_size, self.R, self.W, device=device)\n",
        "        write_weights = torch.zeros(batch_size, self.N, device=device)\n",
        "        return DNCState(controller_h, controller_c, memory, usage, link, precedence, read_weights, read_vectors, write_weights)\n",
        "\n",
        "    def step(self, x_t: torch.Tensor, state: DNCState) -> Tuple[torch.Tensor, DNCState]:\n",
        "        \"\"\"\n",
        "        x_t: (B, input_size)\n",
        "        returns y_t: (B, output_size)\n",
        "        \"\"\"\n",
        "        B = x_t.size(0)\n",
        "        # Controller input: x_t + previous read vectors\n",
        "        ctrl_in = torch.cat([x_t, state.read_vectors.view(B, -1)], dim=-1).unsqueeze(0)  # (1, B, D)\n",
        "        ctrl_out, (h, c) = self.controller(ctrl_in, (state.controller_h, state.controller_c))  # ctrl_out: (1,B,H)\n",
        "        ctrl_out = ctrl_out.squeeze(0)  # (B, H)\n",
        "\n",
        "        # Interface parsing\n",
        "        iface = self.interface_linear(ctrl_out)  # (B, D)\n",
        "        iface_parsed = self._parse_interface(iface, B)\n",
        "\n",
        "        # Write to memory\n",
        "        M, usage, link, precedence, w_w = self._write(\n",
        "            state.memory, state.usage, state.link, state.precedence, state.read_weights, iface_parsed\n",
        "        )\n",
        "\n",
        "        # Read from memory\n",
        "        read_weights, read_vectors = self._read(\n",
        "            M, link, state.read_weights, iface_parsed\n",
        "        )\n",
        "\n",
        "        # Output\n",
        "        out_input = torch.cat([ctrl_out, read_vectors.view(B, -1)], dim=-1)\n",
        "        y_t = self.output_layer(out_input)\n",
        "\n",
        "        new_state = DNCState(h, c, M, usage, link, precedence, read_weights, read_vectors, w_w)\n",
        "        return y_t, new_state\n",
        "\n",
        "    def forward(self, x: torch.Tensor, state: DNCState = None):\n",
        "        \"\"\"\n",
        "        x: (T, B, input_size)\n",
        "        returns y: (T, B, output_size), final_state\n",
        "        \"\"\"\n",
        "        T, B, _ = x.shape\n",
        "        device = x.device\n",
        "        if state is None:\n",
        "            state = self.initial_state(B, device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(T):\n",
        "            y_t, state = self.step(x[t], state)\n",
        "            outputs.append(y_t.unsqueeze(0))\n",
        "        y = torch.cat(outputs, dim=0)\n",
        "        return y, state\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Copy task dataset\n",
        "# ---------------------------\n",
        "\n",
        "def generate_copy_batch(batch_size: int, seq_len: int, bits: int, device: str):\n",
        "    \"\"\"\n",
        "    Inputs: T = seq_len + 1 + seq_len\n",
        "    - Phase 1 (write): seq_len timesteps of random bit vectors (bits) with a delimiter channel OFF\n",
        "    - Delimiter timestep: one-hot delimiter channel ON (all bits zero)\n",
        "    - Phase 2 (read): seq_len timesteps of zeros; model must output the original bit vectors\n",
        "    Input size = bits + 1 (delimiter)\n",
        "    Output size = bits (no delimiter in output)\n",
        "    \"\"\"\n",
        "    # Random sequence: (B, seq_len, bits)\n",
        "    seq = torch.bernoulli(0.5 * torch.ones(batch_size, seq_len, bits, device=device))\n",
        "    # Assemble input\n",
        "    T = seq_len + 1 + seq_len\n",
        "    inp = torch.zeros(T, batch_size, bits + 1, device=device)\n",
        "    out = torch.zeros(T, batch_size, bits, device=device)\n",
        "\n",
        "    # Write phase\n",
        "    inp[:seq_len, :, :bits] = seq\n",
        "    # Delimiter\n",
        "    inp[seq_len, :, bits] = 1.0\n",
        "    # Read phase: zeros input\n",
        "\n",
        "    # Desired output during read phase\n",
        "    out[seq_len + 1:, :, :] = seq\n",
        "\n",
        "    return inp, out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "\n",
        "def train_copy(\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    batch_size=16,\n",
        "    seq_len=10,\n",
        "    bits=8,\n",
        "    mem_n=64,\n",
        "    mem_w=32,\n",
        "    read_heads=2,\n",
        "    controller_hidden=256,\n",
        "    controller_layers=1,\n",
        "    lr=1e-3,\n",
        "    epochs=20000,\n",
        "    grad_clip=10.0,\n",
        "    seed=42,\n",
        "    checkpoint=\"./checkpoints/dnc_copy.pt\",\n",
        "    log_every=100\n",
        "):\n",
        "    set_seed(seed)\n",
        "    os.makedirs(os.path.dirname(checkpoint), exist_ok=True)\n",
        "\n",
        "    input_size = bits + 1\n",
        "    output_size = bits\n",
        "\n",
        "    model = DNC(\n",
        "        input_size=input_size,\n",
        "        output_size=output_size,\n",
        "        controller_hidden=controller_hidden,\n",
        "        controller_layers=controller_layers,\n",
        "        mem_n=mem_n,\n",
        "        mem_w=mem_w,\n",
        "        read_heads=read_heads\n",
        "    ).to(device)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for step in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        x, y_true = generate_copy_batch(batch_size, seq_len, bits, device)\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "        # We only supervise read phase outputs\n",
        "        y_pred_read = y_pred[seq_len + 1:]         # (seq_len, B, bits)\n",
        "        y_true_read = y_true[seq_len + 1:]         # (seq_len, B, bits)\n",
        "\n",
        "        loss = F.binary_cross_entropy_with_logits(y_pred_read, y_true_read)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        opt.step()\n",
        "\n",
        "        if step % log_every == 0 or step == 1:\n",
        "            with torch.no_grad():\n",
        "                preds = (torch.sigmoid(y_pred_read) > 0.5).float()\n",
        "                acc = (preds == y_true_read).float().mean().item()\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                torch.save({\"step\": step, \"state_dict\": model.state_dict()}, checkpoint)\n",
        "            print(f\"[{step:05d}] loss={loss.item():.4f} acc={acc:.3f} best_acc={best_acc:.3f}\")\n",
        "\n",
        "    # Show a sample\n",
        "    with torch.no_grad():\n",
        "        x, y_true = generate_copy_batch(1, seq_len, bits, device)\n",
        "        y_pred, _ = model(x)\n",
        "        y_logits = y_pred[seq_len + 1:].squeeze(1)    # (seq_len, bits)\n",
        "        y_hat = (torch.sigmoid(y_logits) > 0.5).float()\n",
        "        print(\"\\nSample (first 4 timesteps of read phase):\")\n",
        "        print(\"target:\", y_true[seq_len + 1: seq_len + 5, 0].cpu().numpy())\n",
        "        print(\"pred  :\", y_hat[:4].cpu().numpy())\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# CLI\n",
        "# ---------------------------\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description=\"Train a DNC on the copy task.\")\n",
        "    p.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    p.add_argument(\"--batch-size\", type=int, default=16)\n",
        "    p.add_argument(\"--seq-len\", type=int, default=10)\n",
        "    p.add_argument(\"--bits\", type=int, default=8)\n",
        "    p.add_argument(\"--mem-n\", type=int, default=64)\n",
        "    p.add_argument(\"--mem-w\", type=int, default=32)\n",
        "    p.add_argument(\"--read-heads\", type=int, default=2)\n",
        "    p.add_argument(\"--controller-hidden\", type=int, default=256)\n",
        "    p.add_argument(\"--controller-layers\", type=int, default=1)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--epochs\", type=int, default=20000)\n",
        "    p.add_argument(\"--grad-clip\", type=float, default=10.0)\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "    p.add_argument(\"--checkpoint\", type=str, default=\"./checkpoints/dnc_copy.pt\")\n",
        "    p.add_argument(\"--log-every\", type=int, default=100)\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    train_copy(\n",
        "        device=args.device,\n",
        "        batch_size=args.batch_size,\n",
        "        seq_len=args.seq_len,\n",
        "        bits=args.bits,\n",
        "        mem_n=args.mem_n,\n",
        "        mem_w=args.mem_w,\n",
        "        read_heads=args.read_heads,\n",
        "        controller_hidden=args.controller_hidden,\n",
        "        controller_layers=args.controller_layers,\n",
        "        lr=args.lr,\n",
        "        epochs=args.epochs,\n",
        "        grad_clip=args.grad_clip,\n",
        "        seed=args.seed,\n",
        "        checkpoint=args.checkpoint,\n",
        "        log_every=args.log_every\n",
        "    )"
      ]
    }
  ]
}