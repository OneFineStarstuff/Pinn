{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO3MqhJlfS5v1L5l3+sZder",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Pinn/blob/main/Navigation_in_a_Dynamic_GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1SRWW2PYzG_"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def mlp(in_dim, hidden, out_dim, act=nn.ReLU):\n",
        "    layers = []\n",
        "    dims = [in_dim] + hidden + [out_dim]\n",
        "    for i in range(len(dims)-2):\n",
        "        layers += [nn.Linear(dims[i], dims[i+1]), act()]\n",
        "    layers += [nn.Linear(dims[-2], dims[-1])]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class MemoryModule(nn.Module):\n",
        "    def __init__(self, d_model, d_mem, capacity=100):\n",
        "        super().__init__()\n",
        "        self.key_proj = nn.Linear(d_model, d_mem)\n",
        "        self.val_proj = nn.Linear(d_model, d_mem)\n",
        "        self.capacity = capacity\n",
        "        self.register_buffer(\"keys\", torch.zeros(capacity, d_mem))\n",
        "        self.register_buffer(\"vals\", torch.zeros(capacity, d_mem))\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "\n",
        "    def write(self, feat):\n",
        "        K = self.key_proj(feat)\n",
        "        V = self.val_proj(feat)\n",
        "        B = K.size(0)\n",
        "        idx = (torch.arange(B) + self.ptr) % self.capacity\n",
        "        self.keys[idx] = K\n",
        "        self.vals[idx] = V\n",
        "        self.ptr = (self.ptr + B) % self.capacity\n",
        "        if self.ptr == 0: self.full = True\n",
        "\n",
        "    def retrieve(self, feat):\n",
        "        if self.ptr == 0 and not self.full:\n",
        "            return torch.zeros(feat.size(0), self.vals.size(-1)), None\n",
        "        sims = self.key_proj(feat) @ self.keys.T\n",
        "        top_idx = sims.argmax(dim=-1)\n",
        "        ctx = self.vals[top_idx]\n",
        "        return ctx, None\n",
        "\n",
        "class Reasoner(nn.Module):\n",
        "    def __init__(self, d_model, d_reason):\n",
        "        super().__init__()\n",
        "        self.net = mlp(d_model, [d_model], d_reason)\n",
        "\n",
        "    def forward(self, feat):\n",
        "        return self.net(feat)\n",
        "\n",
        "class MemoryConditioner(nn.Module):\n",
        "    def __init__(self, d_model, d_mem, d_reason):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model + d_mem + d_reason, d_model)\n",
        "\n",
        "    def forward(self, feat, mem_ctx, reason_ctx):\n",
        "        x = torch.cat([feat, mem_ctx, reason_ctx], dim=-1)\n",
        "        return torch.tanh(self.proj(x))\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, n_act, d_model=32, d_mem=16, d_reason=8):\n",
        "        super().__init__()\n",
        "        self.encoder = mlp(obs_dim, [64], d_model)\n",
        "        self.memory = MemoryModule(d_model, d_mem)\n",
        "        self.reasoner = Reasoner(d_model, d_reason)\n",
        "        self.conditioner = MemoryConditioner(d_model, d_mem, d_reason)\n",
        "        self.policy = mlp(d_model, [32], n_act)\n",
        "        self.value = mlp(d_model, [32], 1)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        feat = self.encoder(obs)\n",
        "        mem_ctx, _ = self.memory.retrieve(feat)\n",
        "        reason_ctx = self.reasoner(feat)\n",
        "        cond = self.conditioner(feat, mem_ctx, reason_ctx)\n",
        "        logits = self.policy(cond)\n",
        "        value = self.value(cond)\n",
        "        return logits, value\n",
        "\n",
        "    def write_memory(self, feat):\n",
        "        self.memory.write(feat)\n",
        "\n",
        "class MemoryReasonerPlanner:\n",
        "    def __init__(self, actor_critic, gamma=0.99):\n",
        "        self.net = actor_critic\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def plan(self, start_state, transition_fn, rollout_fn, eval_fn, max_expansions=100):\n",
        "        # Priority queue: (neg_score, state, first_action, depth)\n",
        "        frontier = []\n",
        "        visited = set()\n",
        "\n",
        "        # Encode start state for memory/reasoner\n",
        "        obs = torch.tensor([float(start_state)], dtype=torch.float32)\n",
        "        feat = self.net.encoder(obs.unsqueeze(0))\n",
        "        mem_ctx, _ = self.net.memory.retrieve(feat)\n",
        "        reason_ctx = self.net.reasoner(feat)\n",
        "\n",
        "        # Initial expansion\n",
        "        for ns in transition_fn(start_state):\n",
        "            score = rollout_fn(ns)\n",
        "            # Reasoner bias: dot product of reason_ctx with itself as a toy heuristic\n",
        "            reason_bias = reason_ctx.squeeze(0).mean().item()\n",
        "            fused_score = score + 0.1 * reason_bias\n",
        "            heapq.heappush(frontier, (-fused_score, ns, ns, 1))\n",
        "\n",
        "        while frontier and len(visited) < max_expansions:\n",
        "            neg_score, state, first_action, depth = heapq.heappop(frontier)\n",
        "            if state in visited:\n",
        "                continue\n",
        "            visited.add(state)\n",
        "\n",
        "            if eval_fn(state):\n",
        "                print(f\"[Planner] Goal reached at state {state} via first action {first_action}\")\n",
        "                return first_action\n",
        "\n",
        "            # Encode for memory/reasoner at this node\n",
        "            obs = torch.tensor([float(state)], dtype=torch.float32)\n",
        "            feat = self.net.encoder(obs.unsqueeze(0))\n",
        "            mem_ctx, _ = self.net.memory.retrieve(feat)\n",
        "            reason_ctx = self.net.reasoner(feat)\n",
        "\n",
        "            for ns in transition_fn(state):\n",
        "                if ns not in visited:\n",
        "                    score = rollout_fn(ns)\n",
        "                    reason_bias = reason_ctx.squeeze(0).mean().item()\n",
        "                    fused_score = score + 0.1 * reason_bias\n",
        "                    heapq.heappush(frontier, (-fused_score, ns, first_action, depth + 1))\n",
        "\n",
        "        print(\"[Planner] No goal found within expansion limit\")\n",
        "        return None\n",
        "\n",
        "# Dummy functions\n",
        "def dummy_transition_fn(state):\n",
        "    return [state + 1, state - 1]\n",
        "\n",
        "def dummy_rollout_fn(state):\n",
        "    return -abs(state - 5)\n",
        "\n",
        "def dummy_eval_fn(state):\n",
        "    return state == 5\n",
        "\n",
        "# Instantiate with your memory-conditioned actor-critic\n",
        "net = ActorCritic(obs_dim=1, n_act=2, d_model=32, d_mem=16, d_reason=8)\n",
        "planner = MemoryReasonerPlanner(net)\n",
        "\n",
        "start_state = 0\n",
        "next_action = planner.plan(start_state, dummy_transition_fn, dummy_rollout_fn, dummy_eval_fn)\n",
        "print(f\"AGI Agent's planned next action: {next_action}\")"
      ]
    }
  ]
}