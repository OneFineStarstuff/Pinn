{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMiHRi4azQZ9CD5HaqV+B70",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/Pinn/blob/main/self_reflective_cartpole_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL_eDcOeQNq9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Self-Reflective Advantage Actor-Critic on CartPole-v1\n",
        "\n",
        "Features:\n",
        "- Collects episodes into a memory buffer, then \"reflects\" via multi-epoch replay:\n",
        "  1) Critic-only epochs to fit V to returns (stabilize targets).\n",
        "  2) Policy epochs using advantages with entropy regularization.\n",
        "- Reproducibility: seeds, deterministic torch flags (where possible).\n",
        "- Safe defaults: grad clipping, advantage normalization, separate optimizers.\n",
        "- Checkpointing: periodic save for policy and critic.\n",
        "- Minimal dependencies: torch, gymnasium (or gym), numpy.\n",
        "\n",
        "Run:\n",
        "  python self_reflective_cartpole.py\n",
        "\n",
        "Notes:\n",
        "- If you have 'gym' instead of 'gymnasium', the script will fall back automatically.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Gym import with fallback\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "except ImportError:\n",
        "    import gym\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Config and utilities\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    env_id: str = \"CartPole-v1\"\n",
        "    max_episodes: int = 500\n",
        "    episodes_per_batch: int = 8\n",
        "    gamma: float = 0.99\n",
        "    policy_lr: float = 3e-4\n",
        "    critic_lr: float = 5e-4\n",
        "    entropy_coef: float = 0.01\n",
        "    value_coef: float = 0.5\n",
        "    grad_clip: float = 0.5\n",
        "    reflection_epochs_critic: int = 4\n",
        "    reflection_epochs_policy: int = 2\n",
        "    hidden_sizes: tuple = (128, 128)\n",
        "    seed: int = 42\n",
        "    device: str = \"cpu\"  # \"cuda\" if available and desired\n",
        "    log_interval: int = 10\n",
        "    checkpoint_every: int = 100\n",
        "    out_dir: str = \"checkpoints_self_reflect\"\n",
        "\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # Some determinism flags (trade-offs for speed/compat)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def to_tensor(x, device):\n",
        "    return torch.as_tensor(x, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Models\n",
        "# -----------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_sizes=(128, 128), act=nn.Tanh):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = in_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers += [nn.Linear(last, h), act()]\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, out_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions, hidden_sizes=(128, 128)):\n",
        "        super().__init__()\n",
        "        self.backbone = MLP(obs_dim, n_actions, hidden_sizes=hidden_sizes, act=nn.Tanh)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        logits = self.backbone(obs)\n",
        "        return logits  # for Categorical\n",
        "\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_sizes=(128, 128)):\n",
        "        super().__init__()\n",
        "        self.backbone = MLP(obs_dim, 1, hidden_sizes=hidden_sizes, act=nn.Tanh)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return self.backbone(obs).squeeze(-1)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Memory buffer\n",
        "# -----------------------------\n",
        "\n",
        "class TrajectoryBuffer:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.clear()\n",
        "\n",
        "    def clear(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.logps = []\n",
        "        self.values = []\n",
        "        self.episode_returns = []  # per-episode sum for logging\n",
        "\n",
        "    def store_step(self, state, action, reward, done, logp, value):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.logps.append(logp)\n",
        "        self.values.append(value)\n",
        "\n",
        "    def finalize_episode(self, ep_return):\n",
        "        self.episode_returns.append(ep_return)\n",
        "\n",
        "    def as_tensors(self):\n",
        "        states = torch.stack([to_tensor(s, self.device) for s in self.states], dim=0)\n",
        "        actions = torch.as_tensor(self.actions, dtype=torch.int64, device=self.device)\n",
        "        rewards = torch.as_tensor(self.rewards, dtype=torch.float32, device=self.device)\n",
        "        dones = torch.as_tensor(self.dones, dtype=torch.bool, device=self.device)\n",
        "        logps = torch.stack(self.logps, dim=0).to(self.device)\n",
        "        values = torch.as_tensor(self.values, dtype=torch.float32, device=self.device)\n",
        "        return states, actions, rewards, dones, logps, values\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Self-reflective agent\n",
        "# -----------------------------\n",
        "\n",
        "class SelfReflectiveAgent:\n",
        "    def __init__(self, obs_dim, n_actions, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "        self.device = torch.device(cfg.device)\n",
        "        self.policy = PolicyNet(obs_dim, n_actions, cfg.hidden_sizes).to(self.device)\n",
        "        self.critic = ValueNet(obs_dim, cfg.hidden_sizes).to(self.device)\n",
        "\n",
        "        self.pi_opt = optim.Adam(self.policy.parameters(), lr=cfg.policy_lr)\n",
        "        self.vf_opt = optim.Adam(self.critic.parameters(), lr=cfg.critic_lr)\n",
        "\n",
        "        self.buffer = TrajectoryBuffer(self.device)\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act(self, obs):\n",
        "        obs_t = to_tensor(obs, self.device).unsqueeze(0)\n",
        "        logits = self.policy(obs_t)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        logp = dist.log_prob(action).squeeze(0)\n",
        "        value = self.critic(obs_t).squeeze(0)\n",
        "        return int(action.item()), logp, float(value.item())\n",
        "\n",
        "    def compute_returns_and_advantages(self, rewards, dones, values, gamma):\n",
        "        # Episode-wise discounted returns since CartPole is episodic\n",
        "        returns = torch.zeros_like(rewards, device=self.device)\n",
        "        G = 0.0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if dones[t]:\n",
        "                G = 0.0\n",
        "            G = rewards[t] + gamma * G\n",
        "            returns[t] = G\n",
        "\n",
        "        advantages = returns - values.detach()\n",
        "        # Normalize advantages for stability\n",
        "        adv_mean, adv_std = advantages.mean(), advantages.std(unbiased=False) + 1e-8\n",
        "        advantages = (advantages - adv_mean) / adv_std\n",
        "        return returns, advantages\n",
        "\n",
        "    def reflect_and_update(self):\n",
        "        # Prepare batch tensors\n",
        "        states, actions, rewards, dones, logps_old, values = self.buffer.as_tensors()\n",
        "        returns, advantages = self.compute_returns_and_advantages(\n",
        "            rewards, dones, values, self.cfg.gamma\n",
        "        )\n",
        "\n",
        "        # Critic reflection: multiple epochs fitting V to returns\n",
        "        for _ in range(self.cfg.reflection_epochs_critic):\n",
        "            v_pred = self.critic(states)\n",
        "            v_loss = torch.nn.functional.mse_loss(v_pred, returns)\n",
        "            self.vf_opt.zero_grad()\n",
        "            v_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.grad_clip)\n",
        "            self.vf_opt.step()\n",
        "\n",
        "        # After critic stabilizes, recompute values and advantages (optional second look)\n",
        "        with torch.no_grad():\n",
        "            values_refined = self.critic(states)\n",
        "        returns_refined, advantages_refined = self.compute_returns_and_advantages(\n",
        "            rewards, dones, values_refined, self.cfg.gamma\n",
        "        )\n",
        "\n",
        "        # Policy reflection: multiple epochs using refined advantages\n",
        "        for _ in range(self.cfg.reflection_epochs_policy):\n",
        "            logits = self.policy(states)\n",
        "            dist = torch.distributions.Categorical(logits=logits)\n",
        "            logps = dist.log_prob(actions)\n",
        "            entropy = dist.entropy().mean()\n",
        "\n",
        "            policy_loss = -(logps * advantages_refined).mean()\n",
        "            value_loss = torch.nn.functional.mse_loss(self.critic(states), returns_refined)\n",
        "            loss = policy_loss + self.cfg.value_coef * value_loss - self.cfg.entropy_coef * entropy\n",
        "\n",
        "            self.pi_opt.zero_grad()\n",
        "            self.vf_opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.cfg.grad_clip)\n",
        "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.grad_clip)\n",
        "            self.pi_opt.step()\n",
        "            self.vf_opt.step()\n",
        "\n",
        "        # Clear memory after reflection\n",
        "        self.buffer.clear()\n",
        "\n",
        "    def save(self, path_prefix):\n",
        "        os.makedirs(os.path.dirname(path_prefix), exist_ok=True)\n",
        "        torch.save(self.policy.state_dict(), f\"{path_prefix}_policy.pt\")\n",
        "        torch.save(self.critic.state_dict(), f\"{path_prefix}_critic.pt\")\n",
        "\n",
        "    def load(self, path_prefix):\n",
        "        self.policy.load_state_dict(torch.load(f\"{path_prefix}_policy.pt\", map_location=self.device))\n",
        "        self.critic.load_state_dict(torch.load(f\"{path_prefix}_critic.pt\", map_location=self.device))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop\n",
        "# -----------------------------\n",
        "\n",
        "def train(cfg: Config):\n",
        "    set_seed(cfg.seed)\n",
        "    env = gym.make(cfg.env_id)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    agent = SelfReflectiveAgent(obs_dim, n_actions, cfg)\n",
        "\n",
        "    ep_count = 0\n",
        "    best_avg = -math.inf\n",
        "    rolling = []\n",
        "\n",
        "    while ep_count < cfg.max_episodes:\n",
        "        # Collect a batch of episodes\n",
        "        batch_start = ep_count\n",
        "        while ep_count - batch_start < cfg.episodes_per_batch and ep_count < cfg.max_episodes:\n",
        "            obs, info = env.reset(seed=cfg.seed + ep_count) if hasattr(env, \"reset\") and \"gymnasium\" in env.__class__.__module__ else (env.reset(), {})\n",
        "            done = False\n",
        "            ep_return = 0.0\n",
        "\n",
        "            while not done:\n",
        "                action, logp, value = agent.act(obs)\n",
        "                step_out = env.step(action)\n",
        "                if len(step_out) == 5:\n",
        "                    next_obs, reward, terminated, truncated, info = step_out\n",
        "                    done = terminated or truncated\n",
        "                else:\n",
        "                    next_obs, reward, done, info = step_out\n",
        "                agent.buffer.store_step(\n",
        "                    state=to_tensor(obs, agent.device),\n",
        "                    action=action,\n",
        "                    reward=reward,\n",
        "                    done=done,\n",
        "                    logp=logp,\n",
        "                    value=value,\n",
        "                )\n",
        "                obs = next_obs\n",
        "                ep_return += reward\n",
        "\n",
        "            agent.buffer.finalize_episode(ep_return)\n",
        "            rolling.append(ep_return)\n",
        "            if len(rolling) > 50:\n",
        "                rolling.pop(0)\n",
        "            ep_count += 1\n",
        "\n",
        "        # Reflection and updates over the collected batch\n",
        "        agent.reflect_and_update()\n",
        "\n",
        "        # Logging\n",
        "        avg_return = float(np.mean(rolling)) if rolling else 0.0\n",
        "        if ep_count % cfg.log_interval == 0:\n",
        "            print(f\"[{ep_count:4d}] avg_return(50)={avg_return:7.2f}  last_batch_mean={np.mean(agent.buffer.episode_returns) if agent.buffer.episode_returns else float('nan')}\")\n",
        "\n",
        "        # Checkpointing\n",
        "        if ep_count % cfg.checkpoint_every == 0:\n",
        "            agent.save(os.path.join(cfg.out_dir, f\"ep{ep_count}\"))\n",
        "            # Track best by rolling average\n",
        "            if avg_return > best_avg:\n",
        "                best_avg = avg_return\n",
        "                agent.save(os.path.join(cfg.out_dir, \"best\"))\n",
        "\n",
        "    env.close()\n",
        "    print(\"Training complete.\")\n",
        "    return agent\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cfg = Config()\n",
        "    # Auto-select CUDA if available and desired\n",
        "    if torch.cuda.is_available():\n",
        "        cfg.device = \"cuda\"\n",
        "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "    trained_agent = train(cfg)"
      ]
    }
  ]
}